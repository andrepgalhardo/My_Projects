{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install keras --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "JExXzroZ9bLo",
        "outputId": "369eae78-21ec-4582-8376-600dfcf3184c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.0.5-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.7)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.0.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              },
              "id": "1cd28c71056241d1882cb5a41d8f8191"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFG-5WSk88E1",
        "outputId": "7f7bbb7f-8c82-4686-8ad1-df722c17c658"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_nlp\n",
            "  Downloading keras_nlp-0.8.2-py3-none-any.whl (465 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/465.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/465.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-core (from keras_nlp)\n",
            "  Downloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (23.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (2023.12.25)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (13.7.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (0.1.8)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (0.2.0)\n",
            "Collecting tensorflow-text (from keras_nlp)\n",
            "  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras_nlp) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras_nlp) (4.66.2)\n",
            "Collecting namex (from keras-core->keras_nlp)\n",
            "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_nlp) (3.9.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (2.16.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras_nlp) (0.16.1)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras_nlp) (2.15.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras_nlp) (0.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.62.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.15.0)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.13.0->tensorflow-text->keras_nlp) (2.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_nlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_nlp) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_nlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_nlp) (2024.2.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.2.2)\n",
            "Installing collected packages: namex, keras-core, tensorflow-text, keras_nlp\n",
            "Successfully installed keras-core-0.1.7 keras_nlp-0.8.2 namex-0.0.7 tensorflow-text-2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CtV7zUki8jlt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # or \"tensorflow\" or \"torch\"\n",
        "\n",
        "import keras_nlp\n",
        "import keras\n",
        "\n",
        "# Use mixed precision to speed up all training in this guide.\n",
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IA820mb38jlx",
        "outputId": "abe736c3-47b6-4a16-ce4c-9d5de1c79284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.0.5\n",
            "0.8.2\n"
          ]
        }
      ],
      "source": [
        "print(keras.__version__)\n",
        "print(keras_nlp.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJjLslur8jly",
        "outputId": "3eef6fab-115c-4a4b-8389-b4261d45150a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  4059k      0  0:00:20  0:00:20 --:--:-- 3976k\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!# Remove unsupervised examples\n",
        "!rm -r aclImdb/train/unsup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF_PA86I8jlz",
        "outputId": "0f51ddfe-0f31-4f36-f99f-172d84cccf28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b\"I'm sorry but I didn't like this doc very much. I can think of a million ways it could have been better. The people who made it obviously don't have much imagination. The interviews aren't very interesting and no real insight is offered. The footage isn't assembled in a very informative way, either. It's too bad because this is a movie that really deserves spellbinding special features. One thing I'll say is that Isabella Rosselini gets more beautiful the older she gets. All considered, this only gets a '4.'\">, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 16\n",
        "imdb_train = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "imdb_test = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\",\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "# Inspect first review\n",
        "# Format is (review text tensor, label tensor)\n",
        "print(imdb_train.unbatch().take(1).get_single_element())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIb2eZI28jl0",
        "outputId": "af69a2ca-36b0-47f5-a4f7-732d721347fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'loss_scale_optimizer', because it has 4 variables whereas the saved optimizer has 2 variables. \n",
            "  trackable.load_own_variables(weights_store.get(inner_path))\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 0 variables. \n",
            "  trackable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.539,  1.543]], dtype=float16)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "classifier = keras_nlp.models.BertClassifier.from_preset(\"bert_tiny_en_uncased_sst2\")\n",
        "# Note: batched inputs expected so must wrap string in iterable\n",
        "classifier.predict([\"I love modular workflows in keras-nlp!\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T10mupH8jl1",
        "outputId": "440f31c6-fd2f-45fc-eb40-7d3fdb1c3556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - loss: 0.4676 - sparse_categorical_accuracy: 0.7832\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4629317820072174, 0.783519983291626]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "classifier.evaluate(imdb_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP4zFHP18jl3"
      },
      "source": [
        "### Fine tuning a pretrained BERT backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSWjAIiD8jl5",
        "outputId": "a32153a9-c4a7-46e7-9f37-1ec993579fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased/2/download/config.json...\n",
            "100%|██████████| 507/507 [00:00<00:00, 316kB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased/2/download/model.weights.h5...\n",
            "100%|██████████| 16.8M/16.8M [00:02<00:00, 6.96MB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased/2/download/tokenizer.json...\n",
            "100%|██████████| 547/547 [00:00<00:00, 479kB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased/2/download/assets/tokenizer/vocabulary.txt...\n",
            "100%|██████████| 226k/226k [00:00<00:00, 306kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 43ms/step - loss: 0.5178 - sparse_categorical_accuracy: 0.7299 - val_loss: 0.3017 - val_sparse_categorical_accuracy: 0.8726\n",
            "Epoch 2/2\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 24ms/step - loss: 0.2873 - sparse_categorical_accuracy: 0.8804 - val_loss: 0.2867 - val_sparse_categorical_accuracy: 0.8811\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ddf90188520>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "classifier = keras_nlp.models.BertClassifier.from_preset(\n",
        "    \"bert_tiny_en_uncased\",\n",
        "    num_classes=2,\n",
        ")\n",
        "classifier.fit(\n",
        "    imdb_train,\n",
        "    validation_data=imdb_test,\n",
        "    epochs=2,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine tuning with user-controlled preprocessing"
      ],
      "metadata": {
        "id": "_m8gWLzLAieP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
        "    \"bert_tiny_en_uncased\",\n",
        "    sequence_length=512,\n",
        ")\n",
        "\n",
        "# Apply the preprocessor to every sample of train and test data using `map()`.\n",
        "# [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data/AUTOTUNE) and `prefetch()` are options to tune performance, see\n",
        "# https://www.tensorflow.org/guide/data_performance for details.\n",
        "\n",
        "# Note: only call `cache()` if you training data fits in CPU memory!\n",
        "imdb_train_cached = (\n",
        "    imdb_train.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "imdb_test_cached = (\n",
        "    imdb_test.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "classifier = keras_nlp.models.BertClassifier.from_preset(\n",
        "    \"bert_tiny_en_uncased\", preprocessor=None, num_classes=2\n",
        ")\n",
        "classifier.fit(\n",
        "    imdb_train_cached,\n",
        "    validation_data=imdb_test_cached,\n",
        "    epochs=3,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PREBQ0EzAjie",
        "outputId": "b1a85692-2ded-4c72-d22f-6a5b14f54eb3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 35ms/step - loss: 0.5180 - sparse_categorical_accuracy: 0.7291 - val_loss: 0.3150 - val_sparse_categorical_accuracy: 0.8660\n",
            "Epoch 2/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 49ms/step - loss: 0.2893 - sparse_categorical_accuracy: 0.8825 - val_loss: 0.3199 - val_sparse_categorical_accuracy: 0.8689\n",
            "Epoch 3/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 46ms/step - loss: 0.2201 - sparse_categorical_accuracy: 0.9143 - val_loss: 0.3073 - val_sparse_categorical_accuracy: 0.8792\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ddfac61bee0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = keras_nlp.models.BertTokenizer.from_preset(\"bert_tiny_en_uncased\")\n",
        "tokenizer([\"I love modular workflows!\", \"Libraries over frameworks!\"])\n",
        "\n",
        "# Write your own packer or use one of our `Layers`\n",
        "packer = keras_nlp.layers.MultiSegmentPacker(\n",
        "    start_value=tokenizer.cls_token_id,\n",
        "    end_value=tokenizer.sep_token_id,\n",
        "    # Note: This cannot be longer than the preset's `sequence_length`, and there\n",
        "    # is no check for a custom preprocessor!\n",
        "    sequence_length=64,\n",
        ")\n",
        "\n",
        "\n",
        "# This function that takes a text sample `x` and its\n",
        "# corresponding label `y` as input and converts the\n",
        "# text into a format suitable for input into a BERT model.\n",
        "def preprocessor(x, y):\n",
        "    token_ids, segment_ids = packer(tokenizer(x))\n",
        "    x = {\n",
        "        \"token_ids\": token_ids,\n",
        "        \"segment_ids\": segment_ids,\n",
        "        \"padding_mask\": token_ids != 0,\n",
        "    }\n",
        "    return x, y\n",
        "\n",
        "\n",
        "imdb_train_preprocessed = imdb_train.map(preprocessor, tf.data.AUTOTUNE).prefetch(\n",
        "    tf.data.AUTOTUNE\n",
        ")\n",
        "imdb_test_preprocessed = imdb_test.map(preprocessor, tf.data.AUTOTUNE).prefetch(\n",
        "    tf.data.AUTOTUNE\n",
        ")\n",
        "\n",
        "# Preprocessed example\n",
        "print(imdb_train_preprocessed.unbatch().take(1).get_single_element())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmVXU5axAreT",
        "outputId": "b65d662c-916c-4eb9-c013-18439f58319e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'token_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
            "array([  101,  1999,  2023, 13939,  1011,  3768,  8447, 17417,  9289,\n",
            "       16976,  1010,  1996,  2088, 17567,  2013,  1037,  7037, 11629,\n",
            "        1997, 14106,  1998,  6144,  1010,  2021,  2053,  2028,  3849,\n",
            "        2000,  2022,  2205,  5191,  2055,  2009,  1999,  2023,  3694,\n",
            "        1011,  1062,  2143,  2029,  6436,  2015,  2625, 14062,  2000,\n",
            "       13243,  3393,  5470,  2226,  1005,  1055,  3297,  2460,  3117,\n",
            "        2084,  2026,  2388,  1005,  1055,  5869,  8490,  2532, 17974,\n",
            "         102], dtype=int32)>, 'segment_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
            "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "      dtype=int32)>, 'padding_mask': <tf.Tensor: shape=(64,), dtype=bool, numpy=\n",
            "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True])>}, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine tuning with a custom model"
      ],
      "metadata": {
        "id": "BZq0Mvc4DEvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\"bert_tiny_en_uncased\")\n",
        "backbone = keras_nlp.models.BertBackbone.from_preset(\"bert_tiny_en_uncased\")\n",
        "\n",
        "imdb_train_preprocessed = (\n",
        "    imdb_train.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "imdb_test_preprocessed = (\n",
        "    imdb_test.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "backbone.trainable = False\n",
        "inputs = backbone.input\n",
        "sequence = backbone(inputs)[\"sequence_output\"]\n",
        "for _ in range(2):\n",
        "    sequence = keras_nlp.layers.TransformerEncoder(\n",
        "        num_heads=2,\n",
        "        intermediate_dim=512,\n",
        "        dropout=0.1,\n",
        "    )(sequence)\n",
        "# Use [CLS] token output to classify\n",
        "outputs = keras.layers.Dense(2)(sequence[:, backbone.cls_token_index, :])\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.AdamW(5e-5),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    jit_compile=True,\n",
        ")\n",
        "model.summary()\n",
        "model.fit(\n",
        "    imdb_train_preprocessed,\n",
        "    validation_data=imdb_test_preprocessed,\n",
        "    epochs=3,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "7y_Cq9__CiVa",
        "outputId": "c132269d-8352-40b4-902f-345cb32f8a52"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ segment_ids (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bert_backbone             │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │      \u001b[38;5;34m4,385,920\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│ (\u001b[38;5;33mBertBackbone\u001b[0m)            │ \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]            │                │ segment_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                           │                        │                │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m198,272\u001b[0m │ bert_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m198,272\u001b[0m │ transformer_encoder[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_4 (\u001b[38;5;33mGetItem\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ transformer_encoder_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m258\u001b[0m │ get_item_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ segment_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bert_backbone             │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,385,920</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BertBackbone</span>)            │ <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]            │                │ segment_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                           │                        │                │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │ bert_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │ transformer_encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_encoder_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │ get_item_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,782,722\u001b[0m (18.24 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,782,722</span> (18.24 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m396,802\u001b[0m (1.51 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">396,802</span> (1.51 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,385,920\u001b[0m (16.73 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,385,920</span> (16.73 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 52ms/step - loss: 0.6325 - sparse_categorical_accuracy: 0.6497 - val_loss: 0.5104 - val_sparse_categorical_accuracy: 0.7512\n",
            "Epoch 2/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 23ms/step - loss: 0.5045 - sparse_categorical_accuracy: 0.7540 - val_loss: 0.4234 - val_sparse_categorical_accuracy: 0.8040\n",
            "Epoch 3/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 23ms/step - loss: 0.4529 - sparse_categorical_accuracy: 0.7841 - val_loss: 0.4005 - val_sparse_categorical_accuracy: 0.8176\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ddf2a197310>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretraining a backbone model\n",
        "\n",
        "#### Preprocessing"
      ],
      "metadata": {
        "id": "D6XNrBiCGQ7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All BERT `en` models have the same vocabulary, so reuse preprocessor from\n",
        "# \"bert_tiny_en_uncased\"\n",
        "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
        "    \"bert_tiny_en_uncased\",\n",
        "    sequence_length=256,\n",
        ")\n",
        "packer = preprocessor.packer\n",
        "tokenizer = preprocessor.tokenizer\n",
        "\n",
        "# keras.Layer to replace some input tokens with the \"[MASK]\" token\n",
        "masker = keras_nlp.layers.MaskedLMMaskGenerator(\n",
        "    vocabulary_size=tokenizer.vocabulary_size(),\n",
        "    mask_selection_rate=0.25,\n",
        "    mask_selection_length=64,\n",
        "    mask_token_id=tokenizer.token_to_id(\"[MASK]\"),\n",
        "    unselectable_token_ids=[\n",
        "        tokenizer.token_to_id(x) for x in [\"[CLS]\", \"[PAD]\", \"[SEP]\"]\n",
        "    ],\n",
        ")\n",
        "\n",
        "\n",
        "def preprocess(inputs, label):\n",
        "    inputs = preprocessor(inputs)\n",
        "    masked_inputs = masker(inputs[\"token_ids\"])\n",
        "    # Split the masking layer outputs into a (features, labels, and weights)\n",
        "    # tuple that we can use with keras.Model.fit().\n",
        "    features = {\n",
        "        \"token_ids\": masked_inputs[\"token_ids\"],\n",
        "        \"segment_ids\": inputs[\"segment_ids\"],\n",
        "        \"padding_mask\": inputs[\"padding_mask\"],\n",
        "        \"mask_positions\": masked_inputs[\"mask_positions\"],\n",
        "    }\n",
        "    labels = masked_inputs[\"mask_ids\"]\n",
        "    weights = masked_inputs[\"mask_weights\"]\n",
        "    return features, labels, weights\n",
        "\n",
        "\n",
        "pretrain_ds = imdb_train.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
        "    tf.data.AUTOTUNE\n",
        ")\n",
        "pretrain_val_ds = imdb_test.map(\n",
        "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Tokens with ID 103 are \"masked\"\n",
        "print(pretrain_ds.unbatch().take(1).get_single_element())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzszHMZgGPT_",
        "outputId": "c87b02d5-40d9-4cbb-f013-6e0295156e2d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'token_ids': <tf.Tensor: shape=(256,), dtype=int32, numpy=\n",
            "array([  101,  1999,  1996,  5365,  2225,   103,  4446,  2398,  2020,\n",
            "        1037,   103,   103,  2040,  2043,   103,  2234,  2000,  2237,\n",
            "         103,  2112,  6340,  3492,  9532,  1012,  2044,   103,   103,\n",
            "        4439,  2001,  1037,  9479,   103,   103,  1999,  1996,  2154,\n",
            "        2077,   103,  2288,  2000,  2035,  3033,  1997,  1996,  3915,\n",
            "        1012,  1026,   103,   103,  1028,  1026,  7987,  1013,  1028,\n",
            "        1996,  5225,  2869,   103,  2499,  2005,  4446,  5795, 13097,\n",
            "        5684,  2004,  2209,  2011,  4388, 13779,   103,  2053,  2367,\n",
            "         103,  2021, 13779,  2001,  1037,  2158,  1997,  2035,  2449,\n",
            "         103,   103,   103,  1037,   103,  2000,   103,  1998,  5086,\n",
            "        1037,  2327,   103,  2000,  2079,  2009,  1012,  1026,  7987,\n",
            "        1013,  1028,  1026,  7987,  1013,  1028,  2007,   103,  2146,\n",
            "        2448,  1997,   103, 26100,  1998,  1996,  2755,  2008,  1996,\n",
            "       24945,  4370,  2007,  2009,  2005,  1996,   103,  2112,  1010,\n",
            "        2057,  2288,   103,  2113,  2035,  1996,   103,  2869,   103,\n",
            "        2070,  2391,  1012,  1037,   103,  2391, 16781,   103,  2020,\n",
            "        4208,   103,  2035,  1997,  2068,  1010,  2295,  1996,  9625,\n",
            "        2020,  2007, 13779,   103, 16235, 24201,  1005,  1055,  2839,\n",
            "         103,  5149, 20356,  1010,   103,  2193,  1016,  3124,   103,\n",
            "        1996, 14906,  1012,   103,  7987,  1013,   103,  1026,  7987,\n",
            "         103,   103, 16235, 24201,   103,   103,  2530,   103,  2001,\n",
            "       27992,   103,  6315, 26100,  1010,  2009,  1005,  1055,   103,\n",
            "        9467,  2008,  2122,  2024,  2025,  7687,  2062,  2411,  1012,\n",
            "        2763,  2138,   103,  2020,  2589,  1999,  2304,   103,  2317,\n",
            "       10529,  2018,  2023,  2042,  2019,   103,  2265,  1010,   103,\n",
            "        2052,  2031,  2042,  2589,  1999,  1666,  2066, 14753,   103,\n",
            "        1998,  2022,   103,  2004,   103,  2004,  2216,  3065,  2024,\n",
            "        1012,   103,   103,  1040,  2131,   103,  2156,  1037,   103,\n",
            "        2062,   103,  1037,   102], dtype=int32)>, 'segment_ids': <tf.Tensor: shape=(256,), dtype=int32, numpy=\n",
            "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>, 'padding_mask': <tf.Tensor: shape=(256,), dtype=bool, numpy=\n",
            "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True])>, 'mask_positions': <tf.Tensor: shape=(64,), dtype=int64, numpy=\n",
            "array([  5,  10,  11,  14,  18,  25,  26,  31,  32,  37,  44,  47,  48,\n",
            "        57,  69,  72,  81,  82,  83,  85,  87,  88,  92, 106, 110, 123,\n",
            "       128, 132, 134, 139, 141, 142, 145, 156, 162, 166, 170, 174, 177,\n",
            "       180, 181, 184, 185, 187, 190, 192, 197, 203, 209, 214, 216, 221,\n",
            "       224, 229, 230, 233, 236, 237, 238, 244, 245, 248, 251, 253])>}, <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
            "array([ 2216,  5931,  9129,  2027,  1010,  2035,  4446,  2449,  2067,\n",
            "       16197,  3915,  7987,  1013,  2040,  2020,  1012,  1010,  2002,\n",
            "        2018,  3105,  2079,  1998,  3626,  1996,  6315,  2087,  2000,\n",
            "        5225,  2012,  2070,  2466,  3210,  2105,  1998,  5216,  1996,\n",
            "        2007,  1026,  1028,  1013,  1028,  1005,  1055,  3746,  2011,\n",
            "       26100,  1037,  3491,  2027,  1998,  1012,  6788,  2023,  1999,\n",
            "        3609, 16076,  2770,  2004,  2411,  2057,  1005,  2000,  2843,\n",
            "        1997], dtype=int32)>, <tf.Tensor: shape=(64,), dtype=float16, numpy=\n",
            "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float16)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT backbone\n",
        "backbone = keras_nlp.models.BertBackbone(\n",
        "    vocabulary_size=tokenizer.vocabulary_size(),\n",
        "    num_layers=2,\n",
        "    num_heads=2,\n",
        "    hidden_dim=128,\n",
        "    intermediate_dim=512,\n",
        ")\n",
        "\n",
        "# Language modeling head\n",
        "mlm_head = keras_nlp.layers.MaskedLMHead(\n",
        "    token_embedding=backbone.token_embedding,\n",
        ")\n",
        "\n",
        "inputs = {\n",
        "    \"token_ids\": keras.Input(shape=(None,), dtype=tf.int32, name=\"token_ids\"),\n",
        "    \"segment_ids\": keras.Input(shape=(None,), dtype=tf.int32, name=\"segment_ids\"),\n",
        "    \"padding_mask\": keras.Input(shape=(None,), dtype=tf.int32, name=\"padding_mask\"),\n",
        "    \"mask_positions\": keras.Input(shape=(None,), dtype=tf.int32, name=\"mask_positions\"),\n",
        "}\n",
        "\n",
        "# Encoded token sequence\n",
        "sequence = backbone(inputs)[\"sequence_output\"]\n",
        "\n",
        "# Predict an output word for each masked input token.\n",
        "# We use the input token embedding to project from our encoded vectors to\n",
        "# vocabulary logits, which has been shown to improve training efficiency.\n",
        "outputs = mlm_head(sequence, mask_positions=inputs[\"mask_positions\"])\n",
        "\n",
        "# Define and compile our pretraining model.\n",
        "pretraining_model = keras.Model(inputs, outputs)\n",
        "pretraining_model.summary()\n",
        "pretraining_model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.AdamW(learning_rate=5e-4),\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    jit_compile=True,\n",
        ")\n",
        "\n",
        "# Pretrain on IMDB dataset\n",
        "pretraining_model.fit(\n",
        "    pretrain_ds,\n",
        "    validation_data=pretrain_val_ds,\n",
        "    epochs=6,  # Increase to 6 for higher accuracy\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "37SiNKyMGg7w",
        "outputId": "a1de6b04-d717-4a6f-e9f4-5bb74bf8f5fc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ mask_positions            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ segment_ids (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bert_backbone             │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │      \u001b[38;5;34m4,385,920\u001b[0m │ mask_positions[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│ (\u001b[38;5;33mBertBackbone\u001b[0m)            │ \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]            │                │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│                           │                        │                │ segment_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                           │                        │                │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ masked_lm_head            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30522\u001b[0m)    │      \u001b[38;5;34m3,954,106\u001b[0m │ bert_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],   │\n",
              "│ (\u001b[38;5;33mMaskedLMHead\u001b[0m)            │                        │                │ mask_positions[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ mask_positions            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ segment_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bert_backbone             │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,385,920</span> │ mask_positions[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BertBackbone</span>)            │ <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]            │                │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│                           │                        │                │ segment_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                           │                        │                │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ masked_lm_head            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30522</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,954,106</span> │ bert_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaskedLMHead</span>)            │                        │                │ mask_positions[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,433,210\u001b[0m (16.91 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,433,210</span> (16.91 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,433,210\u001b[0m (16.91 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,433,210</span> (16.91 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:576: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 64ms/step - loss: 5.6248 - sparse_categorical_accuracy: 0.0607 - val_loss: 4.9752 - val_sparse_categorical_accuracy: 0.1169\n",
            "Epoch 2/6\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 31ms/step - loss: 4.9935 - sparse_categorical_accuracy: 0.1209 - val_loss: 4.8664 - val_sparse_categorical_accuracy: 0.1344\n",
            "Epoch 3/6\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 30ms/step - loss: 4.8602 - sparse_categorical_accuracy: 0.1413 - val_loss: 4.5435 - val_sparse_categorical_accuracy: 0.1969\n",
            "Epoch 4/6\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 35ms/step - loss: 4.5951 - sparse_categorical_accuracy: 0.1914 - val_loss: 4.2552 - val_sparse_categorical_accuracy: 0.2382\n",
            "Epoch 5/6\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 34ms/step - loss: 4.3458 - sparse_categorical_accuracy: 0.2255 - val_loss: 4.1010 - val_sparse_categorical_accuracy: 0.2537\n",
            "Epoch 6/6\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 31ms/step - loss: 4.1705 - sparse_categorical_accuracy: 0.2455 - val_loss: 3.7268 - val_sparse_categorical_accuracy: 0.2962\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7dde778f5f60>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build and train your own transformer from scratch"
      ],
      "metadata": {
        "id": "huQlagvWJGzW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train custom vocabulary from IMDB data"
      ],
      "metadata": {
        "id": "2_Oj1QiGJQlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
        "    imdb_train.map(lambda x, y: x),\n",
        "    vocabulary_size=20_000,\n",
        "    lowercase=True,\n",
        "    strip_accents=True,\n",
        "    reserved_tokens=[\"[PAD]\", \"[START]\", \"[END]\", \"[MASK]\", \"[UNK]\"],\n",
        ")\n",
        "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocab,\n",
        "    lowercase=True,\n",
        "    strip_accents=True,\n",
        "    oov_token=\"[UNK]\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "cHCZFrA9JK6l"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocess data with a custom tokenizer"
      ],
      "metadata": {
        "id": "s2C6dI3xJitr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "packer = keras_nlp.layers.StartEndPacker(\n",
        "    start_value=tokenizer.token_to_id(\"[START]\"),\n",
        "    end_value=tokenizer.token_to_id(\"[END]\"),\n",
        "    pad_value=tokenizer.token_to_id(\"[PAD]\"),\n",
        "    sequence_length=512,\n",
        ")\n",
        "\n",
        "\n",
        "def preprocess(x, y):\n",
        "    token_ids = packer(tokenizer(x))\n",
        "    return token_ids, y\n",
        "\n",
        "\n",
        "imdb_preproc_train_ds = imdb_train.map(\n",
        "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "imdb_preproc_val_ds = imdb_test.map(\n",
        "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(imdb_preproc_train_ds.unbatch().take(1).get_single_element())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VRLVzCmJYhP",
        "outputId": "abdff88b-2254-4163-ea75-a91b59a35962"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(512,), dtype=int32, numpy=\n",
            "array([    1,   152,   410,   212,   115,  1020,   140,    43,  1648,\n",
            "         104,   110,   106,   146,   145,  1776,   102,   148,    18,\n",
            "         102,   100,    43,   155,   105,   384,   238,   270,  1120,\n",
            "         236,   347,   250,    97,  7782,    18,    32,   101,    19,\n",
            "          34,    32,   101,    19,    34,   105,   100,   128,  2601,\n",
            "         103,    43,   270,    98,  1212,   191,   115,   105,    51,\n",
            "       13316,    17,   130,   188,    13,    16,   111,   113,   119,\n",
            "          99,   207,    43,   110,   105,  3097,   300,   102,    18,\n",
            "           2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0],\n",
            "      dtype=int32)>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Design a tiny transformer"
      ],
      "metadata": {
        "id": "acpjq0IcKfOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_id_input = keras.Input(\n",
        "    shape=(None,),\n",
        "    dtype=\"int32\",\n",
        "    name=\"token_ids\",\n",
        ")\n",
        "outputs = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=len(vocab),\n",
        "    sequence_length=packer.sequence_length,\n",
        "    embedding_dim=64,\n",
        ")(token_id_input)\n",
        "outputs = keras_nlp.layers.TransformerEncoder(\n",
        "    num_heads=2,\n",
        "    intermediate_dim=128,\n",
        "    dropout=0.1,\n",
        ")(outputs)\n",
        "# Use \"[START]\" token to classify\n",
        "outputs = keras.layers.Dense(2)(outputs[:, 0, :])\n",
        "model = keras.Model(\n",
        "    inputs=token_id_input,\n",
        "    outputs=outputs,\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "BK_dWsCFKaoA",
        "outputId": "21003464-0ad9-4503-b216-53a38fde4178"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │       \u001b[38;5;34m1,259,648\u001b[0m │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_encoder_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m33,472\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ get_item_6 (\u001b[38;5;33mGetItem\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m130\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,259,648</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_encoder_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,472</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ get_item_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,293,250\u001b[0m (4.93 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,293,250</span> (4.93 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,293,250\u001b[0m (4.93 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,293,250</span> (4.93 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.AdamW(5e-5),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    jit_compile=True,\n",
        ")\n",
        "model.fit(\n",
        "    imdb_preproc_train_ds,\n",
        "    validation_data=imdb_preproc_val_ds,\n",
        "    epochs=3,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dqTorYVKljC",
        "outputId": "06a3e357-0a4c-4aac-f661-db7e7efc6f2e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 21ms/step - loss: 0.6949 - sparse_categorical_accuracy: 0.5352 - val_loss: 0.5050 - val_sparse_categorical_accuracy: 0.7697\n",
            "Epoch 2/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - loss: 0.4403 - sparse_categorical_accuracy: 0.7940 - val_loss: 0.3318 - val_sparse_categorical_accuracy: 0.8627\n",
            "Epoch 3/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 14ms/step - loss: 0.3080 - sparse_categorical_accuracy: 0.8701 - val_loss: 0.3318 - val_sparse_categorical_accuracy: 0.8565\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ddf2bb77910>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "diY6A-u5KrxY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}